{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32037575",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efbdf5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Path to video and image files\n",
    "video_path = 'task1.mp4'  # Replace with the path to your video\n",
    "output_video_path = 'task1_output_sample.avi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27c2c7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display name on video\n",
    "name_text = \"521H0320_LETRI\"  # Replace with your name\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "font_scale = 1\n",
    "font_color = (0, 255, 0)\n",
    "font_thickness = 2\n",
    "text_position = (10, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4406ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: Couldn't read video stream from file \"task1.mp4\"\n"
     ]
    }
   ],
   "source": [
    "# === Task 1: Video processing ===\n",
    "cap = cv2.VideoCapture(video_path)  # Load video for processing\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')  # Define codec for output video\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))  # Get the frames per second from video\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))  # Get video frame width\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))  # Get video frame height\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))  # Create a video writer for output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a435d301",
   "metadata": {},
   "outputs": [],
   "source": [
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()  # Read each frame\n",
    "    if not ret:\n",
    "        break  # Exit if no more frames\n",
    "\n",
    "    # Light balance using CLAHE\n",
    "    ycrcb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2YCrCb)  # Convert to YCrCb color space\n",
    "    clahe = cv2.createCLAHE(clipLimit=1.0, tileGridSize=(4, 4))  # Create CLAHE object for brightness balance\n",
    "    ycrcb_frame[:, :, 1] = clahe.apply(ycrcb_frame[:, :, 1])  # Apply CLAHE to the Y channel\n",
    "    balanced_frame = cv2.cvtColor(ycrcb_frame, cv2.COLOR_YCrCb2BGR)  # Convert back to BGR color space\n",
    "\n",
    "    # Convert to HSV color space\n",
    "    hsv_frame = cv2.cvtColor(balanced_frame, cv2.COLOR_BGR2HSV)  # Convert to HSV for color masking\n",
    "    hsv_frame = cv2.GaussianBlur(hsv_frame, (5, 5), 0)  # Apply Gaussian blur to reduce noise\n",
    "\n",
    "    # Color threshold for blue traffic signs\n",
    "    lower_blue1 = np.array([100, 100, 100])\n",
    "    upper_blue1 = np.array([110, 255, 255])\n",
    "    lower_blue2 = np.array([110, 100, 100])\n",
    "    upper_blue2 = np.array([130, 255, 255])\n",
    "    blue_mask1 = cv2.inRange(hsv_frame, lower_blue1, upper_blue1)  # Mask for light blue range\n",
    "    blue_mask2 = cv2.inRange(hsv_frame, lower_blue2, upper_blue2)  # Mask for dark blue range\n",
    "    blue_mask = cv2.bitwise_or(blue_mask1, blue_mask2)  # Combine both blue masks\n",
    "\n",
    "    # Color threshold for red traffic signs\n",
    "    lower_red1 = np.array([0, 120, 80])\n",
    "    upper_red1 = np.array([10, 255, 255])\n",
    "    lower_red2 = np.array([160, 120, 80])\n",
    "    upper_red2 = np.array([180, 255, 255])\n",
    "    red_mask1 = cv2.inRange(hsv_frame, lower_red1, upper_red1)  # Mask for lower red range\n",
    "    red_mask2 = cv2.inRange(hsv_frame, lower_red2, upper_red2)  # Mask for upper red range\n",
    "    red_mask = cv2.bitwise_or(red_mask1, red_mask2)  # Combine both red masks\n",
    "    combined_mask = cv2.bitwise_or(blue_mask, red_mask)  # Combine blue and red masks\n",
    "\n",
    "    # Remove small noises in the mask\n",
    "    combined_mask = cv2.erode(combined_mask, None, iterations=1)\n",
    "    combined_mask = cv2.dilate(combined_mask, None, iterations=2)\n",
    "    contours, _ = cv2.findContours(combined_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)  # Find contours\n",
    "\n",
    "    output_frame = frame.copy()  # Make a copy for drawing\n",
    "\n",
    "    for contour in contours:\n",
    "        area = cv2.contourArea(contour)  # Calculate contour area\n",
    "        if area > 2100:\n",
    "            circularity = 4 * np.pi * (area / (cv2.arcLength(contour, True) ** 2))  # Calculate circularity\n",
    "            x, y, w, h = cv2.boundingRect(contour)  # Get bounding box coordinates\n",
    "            aspect_ratio = w / float(h)\n",
    "            # Check if it matches a traffic sign shape and size\n",
    "            if 0.65 < circularity < 1.35 and 0.8 < aspect_ratio < 1.2 and 30 <= w <= 100 and 30 <= h <= 100:\n",
    "                cv2.rectangle(output_frame, (x, y), (x + w, y + h), (0, 255, 0), 2)  # Draw bounding box\n",
    "\n",
    "    cv2.putText(output_frame, name_text, text_position, font, font_scale, font_color, font_thickness)  # Add name\n",
    "    out.write(output_frame)  # Write frame to output video\n",
    "    plt.imshow(cv2.cvtColor(output_frame, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Traffic Sign Detection\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6bb7f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R-CNN-main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
